{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "301e86fe",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Task: Build a Campus FAQ Chatbot using RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebacc31",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Objective:\n",
    "Learn how Retrieval-Augmented Generation (RAG) works by building a small chatbot that answers questions about your college using vector embeddings and a mini vector database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10ac740",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### Step 0: Setup\n",
    "\n",
    "1. Install required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b7c666",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'env (Python 3.13.7)' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '\"c:/Users/diego/OneDrive - Lambton College/AIML/Term 3/Software Tools and Emerging Technologies/SimpleRAG/env/Scripts/python.exe\" -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "%%capture # I don't want to display the installation progress. Capture captures it just in case I need to see what happenned.\n",
    "%pip install streamlit sentence-transformers faiss-cpu numpy pypdf2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5890f8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### Step 1: Prepare the Data\n",
    "\n",
    "Task: Create a small FAQ dataset with at least 5 Q&A pairs.\n",
    "Example:\n",
    "\n",
    "Q: When does the library open?\n",
    "A: The library opens at 8 AM and closes at 8 PM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154146b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2 as pdf2 # PDF handling\n",
    "import numpy as np\n",
    "import streamlit as st # FrontEnd \n",
    "import re # ReGex\n",
    "import faiss # Embeddings Database\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2717dd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Extraction - Collection - Gathering\n",
    "\n",
    "# Import the BoK from the PDF\n",
    "text = ''\n",
    "with open ('./NeuralNetwork.pdf', 'rb') as nn:\n",
    "    reader = pdf2.PdfReader(nn)\n",
    "    text = ' '.join([text.extract_text() for text in reader.pages])\n",
    "\n",
    "print(text[0:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93c70e2",
   "metadata": {},
   "source": [
    "Checkpoint:\n",
    "\n",
    "Students should have a list of questions and answers ready."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466b308d",
   "metadata": {},
   "source": [
    "#### Step 2: Split Text into Chunks\n",
    "\n",
    "Task: Split your FAQ into separate lines to treat each Q&A as a chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af04ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Processing - Cleaning \n",
    "\n",
    "#pattern = r'\\w+\\s*\\(.\\):\\s*(.*?)(?=\\w+\\s*\\(.\\):|$)'\n",
    "pattern = r'RN-\\d+\\s+\\|\\s(.*?)(?=\\s+ID:)'\n",
    "\n",
    "text = text.strip().replace('\\n', ' ').replace('\\t', ' ')\n",
    "chunks = re.findall(pattern, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26037189",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd24419",
   "metadata": {},
   "source": [
    "Checkpoint:\n",
    "\n",
    "Ensure each Q&A is a separate element in a Python list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6cdaec",
   "metadata": {},
   "source": [
    "#### Step 3: Create Embeddings\n",
    "\n",
    "Task: Convert each line to a vector using SentenceTransformer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284eb43d",
   "metadata": {},
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5541b0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118b4344",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c951f3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### Step 4: Build the FAISS Index\n",
    "\n",
    "Task: Store all embeddings in a FAISS vector database.\n",
    "\n",
    "> - The 'all-MiniLM-L6-v2' model is an efficient option for generating sentence embeddings, which are numerical representations of text. These embeddings capture the semantic meaning of the text in 384 dimensions, where each dimension represents a feature of the content, allowing for the comparison and retrieval of similar texts based on their embeddings.\n",
    "- The fiss.IndexFlatL2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a794070a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(np.array(embeddings))\n",
    "print(f'Dimension for each embedding of text: {dimension}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c111bd46",
   "metadata": {},
   "source": [
    "#### Step 5: Query the Database\n",
    "Task: Take a user question, convert it to a vector, and find the most relevant FAQ line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321077d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_question = \"What is a Explain About chain rule\"\n",
    "q_emb = model.encode([user_question])\n",
    "D, I = index.search(np.array(q_emb), k=1)\n",
    "print(chunks[I[0][0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9787d4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### Step 6: Make it Interactive with Streamlit\n",
    "Task: Use Streamlit to create a simple chatbot UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54c5d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "st.title(\"Neural Networks BOK\")\n",
    "user_question = st.text_input(\"Ask your question:\")\n",
    "if user_question:\n",
    "    q_emb = model.encode([user_question])\n",
    "    D, I = index.search(np.array(q_emb), k=1)\n",
    "    st.write(\"Answer:\", chunks[I[0][0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb3d4b0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### Step 7: Reflection\n",
    "\n",
    "Questions for students:\n",
    "\n",
    "**How does the chatbot “understand” the question?**\n",
    "To better undestand this, it is necesary to explain the whole RAG process.\n",
    "1. The backstage part: the BoK is constructed using a document that it is extracted, cleaned and splitted into chunks. For this example the pairs Q&A are used as chunks.\n",
    "2. Embeddings creation: each chuunk is converted into a vector using a pre-trained model called SentenceTransformer - All-mini-v6-l2. This model generates embeddings that capture the semantic meaning of the text in 384 dimensions.\n",
    "3. Vector database: the embeddings are stored in a FAISS vector database, which allows for efficient similarity search. The emebddings are indexed following their positions chunks in the original document.\n",
    "4. The questions is answerd here: the user inputs a question that is converted into an embedding using the same sentemce tranformer model.\n",
    "\n",
    "**What happens if the user asks something not in the FAQ?**\n",
    "Given that, the application will return the most similar FAISS vector stored in the database, which may not be relevant to the user's question. Keep in mind that the retrieval is based on L2 (Euclidean distance) to calculate similarity, so the returned answer might not accurately address the user's query, but the shortest distance vector will be provided.\n",
    "\n",
    "**How could you improve this system to handle more questions or longer documents?**\n",
    "- L2 distancia is good by calculating similarity in an small dataset, however it gets worse when the dataset is larger. In this case, we could use other algorithms like HNSW or IVF to improve the search time and accuracy.\n",
    "- LLM integration: this is a good and a more robust solution as Large Language Models can refine, disregard or even generate new answers based on the retrieved context.\n",
    "-"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simplerag",
   "language": "python",
   "name": "simplerag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
