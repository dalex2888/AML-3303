{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "301e86fe",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Task: Build a Campus FAQ Chatbot using RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebacc31",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Objective:\n",
    "Learn how Retrieval-Augmented Generation (RAG) works by building a small chatbot that answers questions about your college using vector embeddings and a mini vector database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10ac740",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### Step 0: Setup\n",
    "\n",
    "1. Install required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b7c666",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'env (Python 3.13.7)' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '\"c:/Users/diego/OneDrive - Lambton College/AIML/Term 3/Software Tools and Emerging Technologies/SimpleRAG/env/Scripts/python.exe\" -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "%%capture # I don't want to display the installation progress. Capture captures it just in case I need to see what happenned.\n",
    "%pip install streamlit sentence-transformers faiss-cpu numpy pypdf2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5890f8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### Step 1: Prepare the Data\n",
    "\n",
    "Task: Create a small FAQ dataset with at least 5 Q&A pairs.\n",
    "Example:\n",
    "\n",
    "Q: When does the library open?\n",
    "A: The library opens at 8 AM and closes at 8 PM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154146b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2 as pdf2 # PDF handling\n",
    "import numpy as np\n",
    "import streamlit as st # FrontEnd \n",
    "import re # ReGex\n",
    "import faiss # Embeddings Database\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2717dd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Extraction - Collection - Gathering\n",
    "\n",
    "# Import the BoK from the PDF\n",
    "text = ''\n",
    "with open ('./NeuralNetwork.pdf', 'rb') as nn:\n",
    "    reader = pdf2.PdfReader(nn)\n",
    "    text = ' '.join([text.extract_text() for text in reader.pages])\n",
    "\n",
    "print(text[0:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93c70e2",
   "metadata": {},
   "source": [
    "Checkpoint:\n",
    "\n",
    "Students should have a list of questions and answers ready."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466b308d",
   "metadata": {},
   "source": [
    "#### Step 2: Split Text into Chunks\n",
    "\n",
    "Task: Split your FAQ into separate lines to treat each Q&A as a chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af04ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Processing - Cleaning \n",
    "\n",
    "#pattern = r'\\w+\\s*\\(.\\):\\s*(.*?)(?=\\w+\\s*\\(.\\):|$)'\n",
    "pattern = r'RN-\\d+\\s+\\|\\s(.*?)(?=\\s+ID:)'\n",
    "\n",
    "text = text.strip().replace('\\n', ' ').replace('\\t', ' ')\n",
    "chunks = re.findall(pattern, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26037189",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd24419",
   "metadata": {},
   "source": [
    "Checkpoint:\n",
    "\n",
    "Ensure each Q&A is a separate element in a Python list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6cdaec",
   "metadata": {},
   "source": [
    "#### Step 3: Create Embeddings\n",
    "\n",
    "Task: Convert each line to a vector using SentenceTransformer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284eb43d",
   "metadata": {},
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5541b0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118b4344",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c951f3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### Step 4: Build the FAISS Index\n",
    "\n",
    "Task: Store all embeddings in a FAISS vector database.\n",
    "\n",
    "> - The 'all-MiniLM-L6-v2' model is an efficient option for generating sentence embeddings, which are numerical representations of text. These embeddings capture the semantic meaning of the text in 384 dimensions, where each dimension represents a feature of the content, allowing for the comparison and retrieval of similar texts based on their embeddings.\n",
    "- The fiss.IndexFlatL2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a794070a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(np.array(embeddings))\n",
    "print(f'Dimension for each embedding of text: {dimension}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c111bd46",
   "metadata": {},
   "source": [
    "#### Step 5: Query the Database\n",
    "Task: Take a user question, convert it to a vector, and find the most relevant FAQ line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321077d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_question = \"What is a Explain About chain rule\"\n",
    "q_emb = model.encode([user_question])\n",
    "D, I = index.search(np.array(q_emb), k=1)\n",
    "print(chunks[I[0][0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9787d4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### Step 6: Make it Interactive with Streamlit\n",
    "Task: Use Streamlit to create a simple chatbot UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54c5d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "st.title(\"Neural Networks BOK\")\n",
    "user_question = st.text_input(\"Ask your question:\")\n",
    "if user_question:\n",
    "    q_emb = model.encode([user_question])\n",
    "    D, I = index.search(np.array(q_emb), k=1)\n",
    "    st.write(\"Answer:\", chunks[I[0][0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb3d4b0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### Step 7: Reflection\n",
    "\n",
    "Questions for students:\n",
    "\n",
    "##### **How does the chatbot “understand” the question?**\n",
    "\n",
    "To better understand this, it is necessary to explain the whole RAG (Retrieval-Augmented Generation) process.\n",
    "\n",
    "1. **The backstage part:**  \n",
    "   The BoK (Body of Knowledge) is constructed from a source document that is extracted, cleaned, and split into smaller chunks.  \n",
    "   For this example, Q&A pairs are used as chunks.\n",
    "\n",
    "2. **Embeddings creation:**  \n",
    "   Each chunk is converted into a vector using a pre-trained model called *SentenceTransformer – all-MiniLM-L6-v2*.  \n",
    "   This model generates embeddings that capture the semantic meaning of the text in **384 dimensions**.\n",
    "\n",
    "3. **Vector database:**  \n",
    "   These embeddings are stored in a **FAISS** vector database, which allows efficient similarity search.  \n",
    "   The embeddings are indexed according to their position within the original document.\n",
    "\n",
    "4. **Question processing:**  \n",
    "   When the user inputs a question, it is also converted into an embedding using the same SentenceTransformer model.  \n",
    "   The system then searches for the most similar embeddings within the FAISS index to retrieve the most relevant chunk(s) of text.\n",
    "\n",
    "\n",
    "\n",
    "##### **What happens if the user asks something not in the FAQ?**\n",
    "\n",
    "In that case, the application will still return the most similar vector stored in the FAISS database, even if it’s not relevant to the user’s query.  \n",
    "Keep in mind that the retrieval process relies on **L2 (Euclidean) distance** to calculate similarity.  \n",
    "Therefore, while the system always returns the “closest” vector, that doesn’t necessarily mean the answer will accurately address the user’s question — it’s simply the best match numerically.\n",
    "\n",
    "\n",
    "\n",
    "##### **How could you improve this system to handle more questions or longer documents?**\n",
    "\n",
    "- **Optimize similarity search:**  \n",
    "  L2 distance works well for small datasets, but performance degrades as the dataset grows.  \n",
    "  In larger collections, algorithms such as **HNSW (Hierarchical Navigable Small World graphs)** or **IVF (Inverted File Index)** can improve both search time and accuracy.\n",
    "\n",
    "- **Integrate a Large Language Model (LLM):**  \n",
    "  Adding an LLM to the pipeline (e.g., using a *retrieval + generation* approach) makes the system more robust.  \n",
    "  The LLM can refine, combine, or even generate new answers based on the retrieved chunks, reducing irrelevant or incomplete responses.\n",
    "\n",
    "- **Add metadata filtering:**  \n",
    "  Include contextual metadata (e.g., topic, source, date) to allow filtered retrieval, ensuring only relevant document sections are compared.  \n",
    "  This reduces noise and improves the quality of the retrieved results.\n",
    "\n",
    "- **Use hybrid search (semantic + keyword):**  \n",
    "  Combine vector similarity with keyword-based search (like **BM25**).  \n",
    "  This hybrid approach balances semantic understanding with lexical precision, improving relevance for factual or domain-specific queries. (this recommendation answer is AI generated)\n",
    "\n",
    "- **Scale document processing:**  \n",
    "  For longer documents, apply **hierarchical chunking** — dividing texts into sections and sub-sections — and store embeddings at different levels of granularity.  \n",
    "  This allows retrieval at the most contextually appropriate level and supports better scalability.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1acdf63",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simplerag",
   "language": "python",
   "name": "simplerag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
