{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19fc31d4",
   "metadata": {},
   "source": [
    "# **Data Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3819c2",
   "metadata": {},
   "source": [
    "This notebook handles data cleaning, feature engineering, and preparation for modeling based on insights from the exploratory data analysis. The preprocessing pipeline ensures no data leakage by performing train/test split before any statistical operations like outlier removal or scaling.\n",
    "\n",
    "The cleaned dataset will be saved both locally and uploaded to S3 for use in the modeling phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "310c3f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "import io\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a1577945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FETCH DATA FROM AWS\n",
    "\n",
    "# Create the AWS S3 Client\n",
    "s3_client = boto3.client('s3')\n",
    "# Load the data in a dictionary 'response'\n",
    "response = s3_client.get_object(Bucket = 'software-tools-ai', Key ='raw_data/listings.csv')\n",
    "# Filter the content of the csvg\n",
    "csv_content = response['Body'].read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fa3968ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48895, 16)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>host_id</th>\n",
       "      <th>host_name</th>\n",
       "      <th>neighbourhood_group</th>\n",
       "      <th>neighbourhood</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>room_type</th>\n",
       "      <th>price</th>\n",
       "      <th>minimum_nights</th>\n",
       "      <th>number_of_reviews</th>\n",
       "      <th>last_review</th>\n",
       "      <th>reviews_per_month</th>\n",
       "      <th>calculated_host_listings_count</th>\n",
       "      <th>availability_365</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2539</td>\n",
       "      <td>Clean &amp; quiet apt home by the park</td>\n",
       "      <td>2787</td>\n",
       "      <td>John</td>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>Kensington</td>\n",
       "      <td>40.64749</td>\n",
       "      <td>-73.97237</td>\n",
       "      <td>Private room</td>\n",
       "      <td>149</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>2018-10-19</td>\n",
       "      <td>0.21</td>\n",
       "      <td>6</td>\n",
       "      <td>365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2595</td>\n",
       "      <td>Skylit Midtown Castle</td>\n",
       "      <td>2845</td>\n",
       "      <td>Jennifer</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>Midtown</td>\n",
       "      <td>40.75362</td>\n",
       "      <td>-73.98377</td>\n",
       "      <td>Entire home/apt</td>\n",
       "      <td>225</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>2019-05-21</td>\n",
       "      <td>0.38</td>\n",
       "      <td>2</td>\n",
       "      <td>355</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                name  host_id host_name  \\\n",
       "0  2539  Clean & quiet apt home by the park     2787      John   \n",
       "1  2595               Skylit Midtown Castle     2845  Jennifer   \n",
       "\n",
       "  neighbourhood_group neighbourhood  latitude  longitude        room_type  \\\n",
       "0            Brooklyn    Kensington  40.64749  -73.97237     Private room   \n",
       "1           Manhattan       Midtown  40.75362  -73.98377  Entire home/apt   \n",
       "\n",
       "   price  minimum_nights  number_of_reviews last_review  reviews_per_month  \\\n",
       "0    149               1                  9  2018-10-19               0.21   \n",
       "1    225               1                 45  2019-05-21               0.38   \n",
       "\n",
       "   calculated_host_listings_count  availability_365  \n",
       "0                               6               365  \n",
       "1                               2               355  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load fetched data into the dataset\n",
    "df = pd.read_csv(io.BytesIO(csv_content), header = 0, sep = ',')\n",
    "print(df.shape)\n",
    "df.head(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef90a92",
   "metadata": {},
   "source": [
    "### Outlier Handling Strategy\n",
    "\n",
    "Based on EDA findings, price has significant outliers (up to $10,000). This preprocessing pipeline implements an **experimental baseline strategy**: removing listings above the 99th percentile ($799).\n",
    "\n",
    "**This is one of several strategies to be tested during modeling:**\n",
    "- Strategy 1 (this notebook): Remove outliers > 99th percentile\n",
    "- Strategy 2 (modeling phase): Log transformation to compress extremes\n",
    "- Strategy 3 (modeling phase): Keep all data, use robust models (Random Forest, XGBoost)\n",
    "\n",
    "The final outlier handling approach will be selected based on model performance comparison tracked in MLflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4dc63d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset: (48895, 16)\n",
      "After removing $0 prices: (48884, 16)\n",
      "After dropping ID columns: (48884, 11)\n",
      "Remaining columns: ['neighbourhood_group', 'latitude', 'longitude', 'room_type', 'price', 'minimum_nights', 'number_of_reviews', 'last_review', 'reviews_per_month', 'calculated_host_listings_count', 'availability_365']\n"
     ]
    }
   ],
   "source": [
    "# Data cleaning before split\n",
    "\n",
    "# Remove invalid prices\n",
    "print(f\"Original dataset: {df.shape}\")\n",
    "df = df[df['price'] > 0]\n",
    "print(f\"After removing $0 prices: {df.shape}\")\n",
    "\n",
    "# Drop unnecessary columns\n",
    "cols_to_drop = ['id', 'host_id', 'name', 'host_name', 'neighbourhood']\n",
    "df = df.drop(columns=cols_to_drop)\n",
    "print(f\"After dropping ID columns: {df.shape}\")\n",
    "print(f\"Remaining columns: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de180b7d",
   "metadata": {},
   "source": [
    "### Train-Test-Splt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7292d573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 39107 samples\n",
      "Test set: 9777 samples\n",
      "Split ratio: 80.0% train, 20.0% test\n"
     ]
    }
   ],
   "source": [
    "# Separate features and target\n",
    "X = df.drop('price', axis=1)\n",
    "y = df['price']\n",
    "\n",
    "# Split 80/20\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "print(f\"Split ratio: {X_train.shape[0]/len(df)*100:.1f}% train, {X_test.shape[0]/len(df)*100:.1f}% test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85841f3c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2fb76e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outlier threshold (99th percentile from train): $798.76\n",
      "Outliers in training set: 392\n",
      "Training set after outlier removal: 38715 samples\n",
      "Test set (unchanged): 9777 samples\n"
     ]
    }
   ],
   "source": [
    "# Calculate outlier threshold on TRAIN only\n",
    "outlier_threshold = y_train.quantile(0.99)\n",
    "print(f\"Outlier threshold (99th percentile from train): ${outlier_threshold:.2f}\")\n",
    "\n",
    "# Count outliers in train\n",
    "n_outliers_train = (y_train > outlier_threshold).sum()\n",
    "print(f\"Outliers in training set: {n_outliers_train}\")\n",
    "\n",
    "# Remove outliers from TRAIN\n",
    "mask = y_train <= outlier_threshold\n",
    "X_train = X_train[mask]\n",
    "y_train = y_train[mask]\n",
    "\n",
    "print(f\"Training set after outlier removal: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set (unchanged): {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1e8d907f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in training set:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column</th>\n",
       "      <th>missing_count</th>\n",
       "      <th>missing_pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>last_review</td>\n",
       "      <td>7833</td>\n",
       "      <td>20.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>reviews_per_month</td>\n",
       "      <td>7833</td>\n",
       "      <td>20.23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              column  missing_count  missing_pct\n",
       "6        last_review           7833        20.23\n",
       "7  reviews_per_month           7833        20.23"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check missing values in train\n",
    "missing_df = pd.DataFrame({\n",
    "    'column': X_train.columns,\n",
    "    'missing_count': X_train.isnull().sum().values,\n",
    "    'missing_pct': (X_train.isnull().sum() / len(X_train) * 100).round(2).values\n",
    "})\n",
    "\n",
    "missing_df = missing_df[missing_df['missing_count'] > 0].sort_values('missing_count', ascending=False)\n",
    "\n",
    "print(\"Missing values in training set:\")\n",
    "missing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9e8520d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (38715, 9)\n",
      "Test set shape: (9777, 9)\n",
      "\n",
      "Missing values remaining in train: 0\n",
      "Missing values remaining in test: 0\n"
     ]
    }
   ],
   "source": [
    "# Handle missing values\n",
    "\n",
    "# Impute reviews_per_month with 0 (listings without reviews)\n",
    "X_train['reviews_per_month'] = X_train['reviews_per_month'].fillna(0)\n",
    "X_test['reviews_per_month'] = X_test['reviews_per_month'].fillna(0)\n",
    "\n",
    "# Drop last_review (not useful for prediction)\n",
    "X_train = X_train.drop(columns=['last_review'])\n",
    "X_test = X_test.drop(columns=['last_review'])\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")\n",
    "print(f\"\\nMissing values remaining in train: {X_train.isnull().sum().sum()}\")\n",
    "print(f\"Missing values remaining in test: {X_test.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a0b0d8",
   "metadata": {},
   "source": [
    "## Preprocessing Summary (So Far)\n",
    "\n",
    "The dataset has been cleaned and split for modeling. Invalid prices ($0) were removed, reducing the dataset from 48,895 to 48,884 listings. Unnecessary identifier columns (id, host_id, name, host_name) and the neighbourhood column were dropped. Neighbourhood had 221 unique categories which would create high dimensionality with one-hot encoding, so neighbourhood_group (5 boroughs) was kept instead as it captures location patterns more efficiently.\n",
    "\n",
    "After an 80/20 train/test split, outliers above the 99th percentile ($798.76) were removed from the training set only, eliminating 392 listings. This experimental baseline strategy will be compared against alternative approaches (log transformation, robust models) during modeling.\n",
    "\n",
    "Missing values in review columns (~20%) were imputed with 0 for `reviews_per_month` (representing listings without reviews) and `last_review` was dropped as it doesn't provide useful predictive information.\n",
    "\n",
    "**Current dataset:**\n",
    "- Training: 38,715 samples, 9 features\n",
    "- Test: 9,777 samples, 9 features\n",
    "- No missing values remaining\n",
    "\n",
    "Next step is feature engineering to create additional predictive signals from geographic coordinates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5720ec27",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4a3e5f",
   "metadata": {},
   "source": [
    "Based on EDA findings showing Manhattan as the most expensive area, a new feature was created: distance from each listing to Manhattan's center (Times Square coordinates: 40.7580, -73.9855)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d30f5281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance range in train: 0.0007 to 0.3631\n"
     ]
    }
   ],
   "source": [
    "# Manhattan center coordinates\n",
    "manhattan_lat = 40.7580\n",
    "manhattan_lon = -73.9855\n",
    "\n",
    "# Calculate Euclidean distance for train\n",
    "X_train['distance_to_manhattan'] = np.sqrt(\n",
    "    (X_train['latitude'] - manhattan_lat)**2 + \n",
    "    (X_train['longitude'] - manhattan_lon)**2\n",
    ")\n",
    "\n",
    "# Calculate for test\n",
    "X_test['distance_to_manhattan'] = np.sqrt(\n",
    "    (X_test['latitude'] - manhattan_lat)**2 + \n",
    "    (X_test['longitude'] - manhattan_lon)**2\n",
    ")\n",
    "\n",
    "print(f\"Distance range in train: {X_train['distance_to_manhattan'].min():.4f} to {X_train['distance_to_manhattan'].max():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59eb21c",
   "metadata": {},
   "source": [
    "The distance is calculated using Euclidean distance in geographic coordinates (latitude/longitude degrees). While this doesn't account for Earth's curvature, it's sufficient for NYC's small geographic area. The resulting values are in degrees rather than kilometers, but this doesn't affect model performance since what matters is the relative ordering (closer vs farther from Manhattan).\n",
    "\n",
    "Distance range: 0.0007 to 0.3631 degrees, which translates roughly to 80 meters (very close to Times Square) up to ~40km (outer boroughs like Staten Island)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5c15b322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical: ['neighbourhood_group', 'room_type']\n",
      "Numeric: ['latitude', 'longitude', 'minimum_nights', 'number_of_reviews', 'reviews_per_month', 'calculated_host_listings_count', 'availability_365', 'distance_to_manhattan']\n",
      "Total: 10 features\n"
     ]
    }
   ],
   "source": [
    "categorical_cols = ['neighbourhood_group', 'room_type']\n",
    "\n",
    "numeric_cols = ['latitude', 'longitude', 'minimum_nights', \n",
    "                'number_of_reviews', 'reviews_per_month', \n",
    "                'calculated_host_listings_count', 'availability_365',\n",
    "                'distance_to_manhattan']\n",
    "\n",
    "print(f\"Categorical: {categorical_cols}\")\n",
    "print(f\"Numeric: {numeric_cols}\")\n",
    "print(f\"Total: {len(categorical_cols) + len(numeric_cols)} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5c9b6329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_processed shape: (38715, 14) \n",
      "X_test_processed shape: (9777, 14) \n",
      "\n",
      "X_train_processed shape: <class 'numpy.ndarray'> \n",
      "X_test_processed shape: <class 'numpy.ndarray'> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('onehot', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'), categorical_cols),\n",
    "    ('stdscaler', StandardScaler(), numeric_cols)\n",
    "])\n",
    "\n",
    "preprocessor.fit(X_train)\n",
    "X_train_processed = preprocessor.transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "print(f'X_train_processed shape: {X_train_processed.shape} \\nX_test_processed shape: {X_test_processed.shape} \\n')\n",
    "print(f'X_train_processed shape: {type(X_train_processed)} \\nX_test_processed shape: {type(X_test_processed)} \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14e94da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding and Scaling\n",
    "\n",
    "# Define column types\n",
    "categorical_cols = ['neighbourhood_group', 'room_type']\n",
    "numeric_cols = ['latitude', 'longitude', 'minimum_nights', \n",
    "                'number_of_reviews', 'reviews_per_month', \n",
    "                'calculated_host_listings_count', 'availability_365',\n",
    "                'distance_to_manhattan']\n",
    "\n",
    "# Create preprocessing pipeline\n",
    "# - OneHotEncoder: converts categorical variables to binary columns (drop first to avoid multicollinearity)\n",
    "# - StandardScaler: normalizes numeric features to mean=0, std=1\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('onehot', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'), categorical_cols),\n",
    "    ('stdscaler', StandardScaler(), numeric_cols)\n",
    "])\n",
    "\n",
    "# Fit preprocessor on training data (learn statistics)\n",
    "preprocessor.fit(X_train)\n",
    "\n",
    "# Transform both train and test using the same statistics\n",
    "X_train_processed = preprocessor.transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "# Verify shapes and data type\n",
    "print(f'X_train_processed shape: {X_train_processed.shape}')\n",
    "print(f'X_test_processed shape: {X_test_processed.shape}')\n",
    "print(f'\\nX_train_processed type: {type(X_train_processed)}')\n",
    "print(f'X_test_processed type: {type(X_test_processed)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00d766f",
   "metadata": {},
   "source": [
    "### Saving Preprocessed Dataset - S3 and Local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cd380d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/processed\\train_processed.csv\n",
      "✓ Saved locally: ../data/processed\\train_processed.csv\n",
      "✓ Saved locally: ../data/processed\\test_processed.csv\n",
      "✓ Uploaded to S3: s3://software-tools-ai/processed_data/train_processed.csv\n",
      "✓ Uploaded to S3: s3://software-tools-ai/processed_data/test_processed.csv\n",
      "\n",
      "Final datasets:\n",
      "Train: (38715, 15)\n",
      "Test: (9777, 15)\n"
     ]
    }
   ],
   "source": [
    "# Save processed datasets\n",
    "\n",
    "# Convert numpy arrays back to DataFrames and combine with target variable\n",
    "# Note: X_processed has no column names (it's a numpy array after ColumnTransformer)\n",
    "# Create generic feature names\n",
    "feature_names = [f'feature_{i}' for i in range(X_train_processed.shape[1])]\n",
    "\n",
    "# Combine X and y for train\n",
    "train_processed = pd.DataFrame(X_train_processed, columns=feature_names, index=X_train.index)\n",
    "train_processed['price'] = y_train\n",
    "\n",
    "# Combine X and y for test\n",
    "test_processed = pd.DataFrame(X_test_processed, columns=feature_names, index=X_test.index)\n",
    "test_processed['price'] = y_test\n",
    "\n",
    "# Create output directory\n",
    "output_dir = '../data/processed'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save locally\n",
    "train_path = os.path.join(output_dir, 'train_processed.csv')\n",
    "test_path = os.path.join(output_dir, 'test_processed.csv')\n",
    "print(train_path)\n",
    "train_processed.to_csv(train_path, index=False)\n",
    "test_processed.to_csv(test_path, index=False)\n",
    "print(f\"✓ Saved locally: {train_path}\")\n",
    "print(f\"✓ Saved locally: {test_path}\")\n",
    "\n",
    "# Upload to S3\n",
    "for filename, df in [('train_processed.csv', train_processed), ('test_processed.csv', test_processed)]:\n",
    "    csv_buffer = io.StringIO()\n",
    "    df.to_csv(csv_buffer, index=False)\n",
    "    \n",
    "    s3_client.put_object(\n",
    "        Bucket='software-tools-ai',\n",
    "        Key=f'processed_data/{filename}',\n",
    "        Body=csv_buffer.getvalue()\n",
    "    )\n",
    "    print(f\"✓ Uploaded to S3: s3://software-tools-ai/processed_data/{filename}\")\n",
    "\n",
    "print(f\"\\nFinal datasets:\")\n",
    "print(f\"Train: {train_processed.shape}\")\n",
    "print(f\"Test: {test_processed.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc244d28",
   "metadata": {},
   "source": [
    "## Preprocessing Complete\n",
    "\n",
    "The dataset has been successfully preprocessed and is ready for modeling. The pipeline applied:\n",
    "\n",
    "**Data cleaning:**\n",
    "- Removed 11 listings with price = $0\n",
    "- Dropped identifier columns and neighbourhood (high cardinality)\n",
    "\n",
    "**Train/test split:**\n",
    "- 80/20 split (38,715 train / 9,777 test samples)\n",
    "- Outliers removed from train only (392 listings above $798.76)\n",
    "\n",
    "**Feature engineering:**\n",
    "- Created distance_to_manhattan feature from geographic coordinates\n",
    "- Imputed missing review values with 0\n",
    "\n",
    "**Encoding and scaling:**\n",
    "- One-hot encoded categorical variables (neighbourhood_group, room_type) with drop_first\n",
    "- StandardScaler applied to numeric features\n",
    "- All transformations fit on train data only to prevent leakage\n",
    "\n",
    "**Final datasets:**\n",
    "- Training: 38,715 samples × 15 features (14 predictors + price)\n",
    "- Test: 9,777 samples × 15 features\n",
    "- Saved locally: `data/processed/train_processed.csv`, `test_processed.csv`\n",
    "- Uploaded to S3: `s3://software-tools-ai/processed_data/`\n",
    "\n",
    "Next step: Model development with MLflow experiment tracking."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2480dfdc",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "airbnbpredictors3",
   "language": "python",
   "name": "airbnbpredictors3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
