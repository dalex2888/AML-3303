{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19fc31d4",
   "metadata": {},
   "source": [
    "# **Data Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3819c2",
   "metadata": {},
   "source": [
    "This notebook handles data cleaning, feature engineering, and preparation for modeling based on insights from the exploratory data analysis. The preprocessing pipeline ensures no data leakage by performing train/test split before any statistical operations like outlier removal or scaling.\n",
    "\n",
    "The cleaned dataset will be saved both locally and uploaded to S3 for use in the modeling phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "310c3f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "import io\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1577945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FETCH DATA FROM AWS\n",
    "\n",
    "# Create the AWS S3 Client\n",
    "s3_client = boto3.client('s3')\n",
    "# Load the data in a dictionary 'response'\n",
    "response = s3_client.get_object(Bucket = 'software-tools-ai', Key ='raw_data/listings.csv')\n",
    "# Filter the content of the csvg\n",
    "csv_content = response['Body'].read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa3968ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48895, 16)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>host_id</th>\n",
       "      <th>host_name</th>\n",
       "      <th>neighbourhood_group</th>\n",
       "      <th>neighbourhood</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>room_type</th>\n",
       "      <th>price</th>\n",
       "      <th>minimum_nights</th>\n",
       "      <th>number_of_reviews</th>\n",
       "      <th>last_review</th>\n",
       "      <th>reviews_per_month</th>\n",
       "      <th>calculated_host_listings_count</th>\n",
       "      <th>availability_365</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2539</td>\n",
       "      <td>Clean &amp; quiet apt home by the park</td>\n",
       "      <td>2787</td>\n",
       "      <td>John</td>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>Kensington</td>\n",
       "      <td>40.64749</td>\n",
       "      <td>-73.97237</td>\n",
       "      <td>Private room</td>\n",
       "      <td>149</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>2018-10-19</td>\n",
       "      <td>0.21</td>\n",
       "      <td>6</td>\n",
       "      <td>365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2595</td>\n",
       "      <td>Skylit Midtown Castle</td>\n",
       "      <td>2845</td>\n",
       "      <td>Jennifer</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>Midtown</td>\n",
       "      <td>40.75362</td>\n",
       "      <td>-73.98377</td>\n",
       "      <td>Entire home/apt</td>\n",
       "      <td>225</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>2019-05-21</td>\n",
       "      <td>0.38</td>\n",
       "      <td>2</td>\n",
       "      <td>355</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                name  host_id host_name  \\\n",
       "0  2539  Clean & quiet apt home by the park     2787      John   \n",
       "1  2595               Skylit Midtown Castle     2845  Jennifer   \n",
       "\n",
       "  neighbourhood_group neighbourhood  latitude  longitude        room_type  \\\n",
       "0            Brooklyn    Kensington  40.64749  -73.97237     Private room   \n",
       "1           Manhattan       Midtown  40.75362  -73.98377  Entire home/apt   \n",
       "\n",
       "   price  minimum_nights  number_of_reviews last_review  reviews_per_month  \\\n",
       "0    149               1                  9  2018-10-19               0.21   \n",
       "1    225               1                 45  2019-05-21               0.38   \n",
       "\n",
       "   calculated_host_listings_count  availability_365  \n",
       "0                               6               365  \n",
       "1                               2               355  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load fetched data into the dataset\n",
    "df = pd.read_csv(io.BytesIO(csv_content), header = 0, sep = ',')\n",
    "print(df.shape)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef90a92",
   "metadata": {},
   "source": [
    "### Outlier Handling Strategy\n",
    "\n",
    "Based on EDA findings, price has significant outliers (up to $10,000). This preprocessing pipeline implements removing listings above the 99th percentile from the training set only.\n",
    "\n",
    "**Outlier handling for features (not target):** Winsorization will be tested during modeling phase as it can improve linear model performance while not affecting tree-based models negatively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dc63d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset: (48895, 16)\n",
      "After removing $0 prices: (48884, 16)\n",
      "After dropping ID columns: (48884, 12)\n",
      "Remaining columns: ['neighbourhood_group', 'neighbourhood', 'latitude', 'longitude', 'room_type', 'price', 'minimum_nights', 'number_of_reviews', 'last_review', 'reviews_per_month', 'calculated_host_listings_count', 'availability_365']\n"
     ]
    }
   ],
   "source": [
    "# Data cleaning before split\n",
    "\n",
    "# Remove invalid prices\n",
    "print(f\"Original dataset: {df.shape}\")\n",
    "df = df[df['price'] > 0]\n",
    "print(f\"After removing $0 prices: {df.shape}\")\n",
    "\n",
    "# Drop unnecessary columns\n",
    "# Note: neighbourhood kept for target encoding, will be dropped later\n",
    "cols_to_drop = ['id', 'host_id', 'name', 'host_name']\n",
    "df = df.drop(columns=cols_to_drop)\n",
    "print(f\"After dropping ID columns: {df.shape}\")\n",
    "print(f\"Remaining columns: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de180b7d",
   "metadata": {},
   "source": [
    "### Train-Test-Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7292d573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 39107 samples\n",
      "Test set: 9777 samples\n",
      "Split ratio: 80.0% train, 20.0% test\n"
     ]
    }
   ],
   "source": [
    "# Separate features and target\n",
    "X = df.drop('price', axis=1)\n",
    "y = df['price']\n",
    "\n",
    "# Split 80/20\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "print(f\"Split ratio: {X_train.shape[0]/len(df)*100:.1f}% train, {X_test.shape[0]/len(df)*100:.1f}% test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85841f3c",
   "metadata": {},
   "source": [
    "### Outlier Removal - Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fb76e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outlier threshold (99th percentile from train): $798.76\n",
      "Outliers in training set: 392\n",
      "Training set after outlier removal: 38715 samples\n",
      "Test set (unchanged): 9777 samples\n"
     ]
    }
   ],
   "source": [
    "# Calculate outlier threshold on TRAIN only\n",
    "outlier_threshold = y_train.quantile(0.99)\n",
    "print(f\"Outlier threshold (99th percentile from train): ${outlier_threshold:.2f}\")\n",
    "\n",
    "# Count outliers in train\n",
    "n_outliers_train = (y_train > outlier_threshold).sum()\n",
    "print(f\"Outliers in training set: {n_outliers_train}\")\n",
    "\n",
    "# Remove outliers from TRAIN\n",
    "mask = y_train <= outlier_threshold\n",
    "X_train = X_train[mask]\n",
    "y_train = y_train[mask]\n",
    "\n",
    "print(f\"Training set after outlier removal: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set (unchanged): {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "missing_values_section",
   "metadata": {},
   "source": [
    "### Missing Value Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e8d907f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in training set:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column</th>\n",
       "      <th>missing_count</th>\n",
       "      <th>missing_pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>last_review</td>\n",
       "      <td>7833</td>\n",
       "      <td>20.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>reviews_per_month</td>\n",
       "      <td>7833</td>\n",
       "      <td>20.23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              column  missing_count  missing_pct\n",
       "7        last_review           7833        20.23\n",
       "8  reviews_per_month           7833        20.23"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check missing values in train\n",
    "missing_df = pd.DataFrame({\n",
    "    'column': X_train.columns,\n",
    "    'missing_count': X_train.isnull().sum().values,\n",
    "    'missing_pct': (X_train.isnull().sum() / len(X_train) * 100).round(2).values\n",
    "})\n",
    "\n",
    "missing_df = missing_df[missing_df['missing_count'] > 0].sort_values('missing_count', ascending=False)\n",
    "\n",
    "print(\"Missing values in training set:\")\n",
    "missing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e8520d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (38715, 10)\n",
      "Test set shape: (9777, 10)\n",
      "\n",
      "Missing values remaining in train: 0\n",
      "Missing values remaining in test: 0\n"
     ]
    }
   ],
   "source": [
    "# Handle missing values\n",
    "\n",
    "# Impute reviews_per_month with 0 (listings without reviews)\n",
    "X_train['reviews_per_month'] = X_train['reviews_per_month'].fillna(0)\n",
    "X_test['reviews_per_month'] = X_test['reviews_per_month'].fillna(0)\n",
    "\n",
    "# Drop last_review (not useful for prediction)\n",
    "X_train = X_train.drop(columns=['last_review'])\n",
    "X_test = X_test.drop(columns=['last_review'])\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")\n",
    "print(f\"\\nMissing values remaining in train: {X_train.isnull().sum().sum()}\")\n",
    "print(f\"Missing values remaining in test: {X_test.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5720ec27",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4a3e5f",
   "metadata": {},
   "source": [
    "### Geographic Distance\n",
    "\n",
    "Based on EDA findings showing Manhattan as the most expensive area, a new feature was created: distance from each listing to Manhattan's center (Times Square coordinates: 40.7580, -73.9855)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d30f5281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance range in train: 0.0007 to 0.3631\n"
     ]
    }
   ],
   "source": [
    "# Manhattan center coordinates\n",
    "manhattan_lat = 40.7580\n",
    "manhattan_lon = -73.9855\n",
    "\n",
    "# Calculate Euclidean distance for train\n",
    "X_train['distance_to_manhattan'] = np.sqrt(\n",
    "    (X_train['latitude'] - manhattan_lat)**2 + \n",
    "    (X_train['longitude'] - manhattan_lon)**2\n",
    ")\n",
    "\n",
    "# Calculate for test\n",
    "X_test['distance_to_manhattan'] = np.sqrt(\n",
    "    (X_test['latitude'] - manhattan_lat)**2 + \n",
    "    (X_test['longitude'] - manhattan_lon)**2\n",
    ")\n",
    "\n",
    "print(f\"Distance range in train: {X_train['distance_to_manhattan'].min():.4f} to {X_train['distance_to_manhattan'].max():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "target_encoding_section",
   "metadata": {},
   "source": [
    "### Target Encoding - Neighbourhood\n",
    "\n",
    "The neighbourhood column (221 unique categories) contains valuable pricing information. Instead of one-hot encoding (which would create 221 features), we use target encoding: replacing each neighbourhood with the mean price of listings in that neighbourhood.\n",
    "\n",
    "**Critical:** Statistics are calculated ONLY from training data to prevent data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "target_encoding_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique neighbourhoods: 217\n",
      "\n",
      "Top 5 most expensive neighbourhoods:\n",
      "neighbourhood\n",
      "Tribeca              299.070866\n",
      "NoHo                 287.390625\n",
      "Flatiron District    262.048387\n",
      "Midtown              253.051452\n",
      "Willowbrook          249.000000\n",
      "Name: price, dtype: float64\n",
      "\n",
      "Top 5 least expensive neighbourhoods:\n",
      "neighbourhood\n",
      "Westerleigh    40.000000\n",
      "Mount Eden     40.200000\n",
      "Bull's Head    48.800000\n",
      "Hunts Point    51.333333\n",
      "Soundview      51.916667\n",
      "Name: price, dtype: float64\n",
      "\n",
      "Warning: 4 test samples had neighbourhoods not seen in train\n",
      "Filled with global mean: $137.38\n",
      "\n",
      "Target encoding complete:\n",
      "Train range: $40.00 to $299.07\n",
      "Test range: $40.00 to $299.07\n"
     ]
    }
   ],
   "source": [
    "# Calculate mean price per neighbourhood in TRAIN only\n",
    "neighbourhood_means = X_train.join(y_train).groupby('neighbourhood')['price'].mean()\n",
    "\n",
    "print(f\"Number of unique neighbourhoods: {len(neighbourhood_means)}\")\n",
    "print(f\"\\nTop 5 most expensive neighbourhoods:\")\n",
    "print(neighbourhood_means.sort_values(ascending=False).head())\n",
    "print(f\"\\nTop 5 least expensive neighbourhoods:\")\n",
    "print(neighbourhood_means.sort_values(ascending=True).head())\n",
    "\n",
    "# Map to train\n",
    "X_train['neighbourhood_price_encoded'] = X_train['neighbourhood'].map(neighbourhood_means)\n",
    "\n",
    "# Map to test using SAME statistics from train\n",
    "X_test['neighbourhood_price_encoded'] = X_test['neighbourhood'].map(neighbourhood_means)\n",
    "\n",
    "# Handle neighbourhoods in test that weren't in train (use global mean)\n",
    "global_mean = y_train.mean()\n",
    "n_missing = X_test['neighbourhood_price_encoded'].isnull().sum()\n",
    "if n_missing > 0:\n",
    "    print(f\"\\nWarning: {n_missing} test samples had neighbourhoods not seen in train\")\n",
    "    print(f\"Filled with global mean: ${global_mean:.2f}\")\n",
    "    # FIX: Don't use inplace=True, assign directly\n",
    "    X_test['neighbourhood_price_encoded'] = X_test['neighbourhood_price_encoded'].fillna(global_mean)\n",
    "else:\n",
    "    print(f\"\\n✓ All test neighbourhoods were present in train data\")\n",
    "\n",
    "print(f\"\\nTarget encoding complete:\")\n",
    "print(f\"Train range: ${X_train['neighbourhood_price_encoded'].min():.2f} to ${X_train['neighbourhood_price_encoded'].max():.2f}\")\n",
    "print(f\"Test range: ${X_test['neighbourhood_price_encoded'].min():.2f} to ${X_test['neighbourhood_price_encoded'].max():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interaction_features_section",
   "metadata": {},
   "source": [
    "### Interaction Features\n",
    "\n",
    "Create interaction between room_type and neighbourhood_group to capture patterns like \"entire homes in Manhattan are more expensive than entire homes elsewhere\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "interaction_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction feature created: 15 unique combinations\n",
      "\n",
      "Examples:\n",
      "room_borough\n",
      "Entire home/apt_Manhattan    10341\n",
      "Private room_Brooklyn         8089\n",
      "Entire home/apt_Brooklyn      7578\n",
      "Private room_Manhattan        6312\n",
      "Private room_Queens           2685\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create interaction feature\n",
    "X_train['room_borough'] = X_train['room_type'] + '_' + X_train['neighbourhood_group']\n",
    "X_test['room_borough'] = X_test['room_type'] + '_' + X_test['neighbourhood_group']\n",
    "\n",
    "print(f\"Interaction feature created: {X_train['room_borough'].nunique()} unique combinations\")\n",
    "print(f\"\\nExamples:\")\n",
    "print(X_train['room_borough'].value_counts().head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "drop_neighbourhood_section",
   "metadata": {},
   "source": [
    "### Drop Original Neighbourhood Column\n",
    "\n",
    "Now that we've extracted the pricing information via target encoding, we can drop the original neighbourhood column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "drop_neighbourhood_code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (38715, 12)\n",
      "Test set shape: (9777, 12)\n",
      "\n",
      "Remaining columns: ['neighbourhood_group', 'latitude', 'longitude', 'room_type', 'minimum_nights', 'number_of_reviews', 'reviews_per_month', 'calculated_host_listings_count', 'availability_365', 'distance_to_manhattan', 'neighbourhood_price_encoded', 'room_borough']\n"
     ]
    }
   ],
   "source": [
    "# Drop neighbourhood (already encoded)\n",
    "X_train = X_train.drop(columns=['neighbourhood'])\n",
    "X_test = X_test.drop(columns=['neighbourhood'])\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")\n",
    "print(f\"\\nRemaining columns: {list(X_train.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outlier_features_section",
   "metadata": {},
   "source": [
    "### Feature Outlier Handling\n",
    "\n",
    "**Note:** Winsorization of numeric features (clipping at 1st and 99th percentiles) will be tested during the modeling phase. This transformation can improve linear model performance but doesn't affect tree-based models. It will be logged as a separate experiment in MLflow to compare results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "encoding_section",
   "metadata": {},
   "source": [
    "## Encoding and Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "define_columns",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical: ['neighbourhood_group', 'room_type', 'room_borough']\n",
      "Numeric: ['latitude', 'longitude', 'minimum_nights', 'number_of_reviews', 'reviews_per_month', 'calculated_host_listings_count', 'availability_365', 'distance_to_manhattan', 'neighbourhood_price_encoded']\n",
      "Total: 12 features\n"
     ]
    }
   ],
   "source": [
    "# Define column types\n",
    "categorical_cols = ['neighbourhood_group', 'room_type', 'room_borough']\n",
    "\n",
    "numeric_cols = ['latitude', 'longitude', 'minimum_nights', \n",
    "                'number_of_reviews', 'reviews_per_month', \n",
    "                'calculated_host_listings_count', 'availability_365',\n",
    "                'distance_to_manhattan', 'neighbourhood_price_encoded']\n",
    "\n",
    "print(f\"Categorical: {categorical_cols}\")\n",
    "print(f\"Numeric: {numeric_cols}\")\n",
    "print(f\"Total: {len(categorical_cols) + len(numeric_cols)} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c9b6329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_processed shape: (38715, 29)\n",
      "X_test_processed shape: (9777, 29)\n",
      "\n",
      "X_train_processed type: <class 'numpy.ndarray'>\n",
      "X_test_processed type: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# Create preprocessing pipeline\n",
    "# - OneHotEncoder: converts categorical variables to binary columns (drop first to avoid multicollinearity)\n",
    "# - StandardScaler: normalizes numeric features to mean=0, std=1\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('onehot', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'), categorical_cols),\n",
    "    ('stdscaler', StandardScaler(), numeric_cols)\n",
    "])\n",
    "\n",
    "# Fit preprocessor on training data (learn statistics)\n",
    "preprocessor.fit(X_train)\n",
    "\n",
    "# Transform both train and test using the same statistics\n",
    "X_train_processed = preprocessor.transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "print(f'X_train_processed shape: {X_train_processed.shape}')\n",
    "print(f'X_test_processed shape: {X_test_processed.shape}')\n",
    "print(f'\\nX_train_processed type: {type(X_train_processed)}')\n",
    "print(f'X_test_processed type: {type(X_test_processed)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00d766f",
   "metadata": {},
   "source": [
    "### Saving Preprocessed Dataset - S3 and Local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd380d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved locally: ../data/processed\\train_processed_v2.csv\n",
      "✓ Saved locally: ../data/processed\\test_processed_v2.csv\n",
      "✓ Uploaded to S3: s3://software-tools-ai/processed_data/train_processed_v2.csv\n",
      "✓ Uploaded to S3: s3://software-tools-ai/processed_data/test_processed_v2.csv\n",
      "\n",
      "Final datasets:\n",
      "Train: (38715, 30)\n",
      "Test: (9777, 30)\n"
     ]
    }
   ],
   "source": [
    "# Save processed datasets\n",
    "\n",
    "# Convert numpy arrays back to DataFrames and combine with target variable\n",
    "# Note: X_processed has no column names (it's a numpy array after ColumnTransformer)\n",
    "# Create generic feature names\n",
    "feature_names = [f'feature_{i}' for i in range(X_train_processed.shape[1])]\n",
    "\n",
    "# Combine X and y for train\n",
    "train_processed = pd.DataFrame(X_train_processed, columns=feature_names, index=X_train.index)\n",
    "train_processed['price'] = y_train\n",
    "\n",
    "# Combine X and y for test\n",
    "test_processed = pd.DataFrame(X_test_processed, columns=feature_names, index=X_test.index)\n",
    "test_processed['price'] = y_test\n",
    "\n",
    "# Create output directory\n",
    "output_dir = '../data/processed'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save locally\n",
    "train_path = os.path.join(output_dir, 'train_processed_v2.csv')\n",
    "test_path = os.path.join(output_dir, 'test_processed_v2.csv')\n",
    "\n",
    "train_processed.to_csv(train_path, index=False)\n",
    "test_processed.to_csv(test_path, index=False)\n",
    "print(f\"✓ Saved locally: {train_path}\")\n",
    "print(f\"✓ Saved locally: {test_path}\")\n",
    "\n",
    "# Upload to S3\n",
    "for filename, df_data in [('train_processed_v2.csv', train_processed), ('test_processed_v2.csv', test_processed)]:\n",
    "    csv_buffer = io.StringIO()\n",
    "    df_data.to_csv(csv_buffer, index=False)\n",
    "    \n",
    "    s3_client.put_object(\n",
    "        Bucket='software-tools-ai',\n",
    "        Key=f'processed_data/{filename}',\n",
    "        Body=csv_buffer.getvalue()\n",
    "    )\n",
    "    print(f\"✓ Uploaded to S3: s3://software-tools-ai/processed_data/{filename}\")\n",
    "\n",
    "print(f\"\\nFinal datasets:\")\n",
    "print(f\"Train: {train_processed.shape}\")\n",
    "print(f\"Test: {test_processed.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc244d28",
   "metadata": {},
   "source": [
    "## Preprocessing Complete\n",
    "\n",
    "The dataset has been successfully preprocessed and is ready for modeling. The pipeline applied:\n",
    "\n",
    "**Data cleaning:**\n",
    "- Removed 11 listings with price = $0\n",
    "- Dropped identifier columns (id, host_id, name, host_name)\n",
    "\n",
    "**Train/test split:**\n",
    "- 80/20 split (38,715 train / 9,777 test samples after outlier removal)\n",
    "- Outliers removed from train only (392 listings above $798.76)\n",
    "\n",
    "**Feature engineering:**\n",
    "- Created distance_to_manhattan from geographic coordinates\n",
    "- Target encoded neighbourhood (221 categories) using train statistics only\n",
    "- Created room_type × neighbourhood_group interaction features\n",
    "- Imputed missing review values with 0\n",
    "\n",
    "**Encoding and scaling:**\n",
    "- One-hot encoded categorical variables (neighbourhood_group, room_type, room_borough) with drop_first\n",
    "- StandardScaler applied to numeric features including neighbourhood_price_encoded\n",
    "- All transformations fit on train data only to prevent leakage\n",
    "\n",
    "**Final datasets (V2):**\n",
    "- Training: 38,715 samples × (features + price)\n",
    "- Test: 9,777 samples × (features + price)\n",
    "- Saved locally: `data/processed/train_processed_v2.csv`, `test_processed_v2.csv`\n",
    "- Uploaded to S3: `s3://software-tools-ai/processed_data/`\n",
    "\n",
    "**Feature count breakdown:**\n",
    "- One-hot encoded: ~16 features (neighbourhood_group + room_type + room_borough interactions)\n",
    "- Numeric scaled: 9 features (including neighbourhood_price_encoded)\n",
    "- Total: ~25 predictor features + price\n",
    "\n",
    "**Note:** Feature outlier handling (winsorization) will be tested as an experiment during modeling phase.\n",
    "\n",
    "Next step: Model development with MLflow experiment tracking."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "airbnbpredictors3",
   "language": "python",
   "name": "airbnbpredictors3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
